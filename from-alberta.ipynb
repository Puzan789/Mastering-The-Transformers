{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- The Albert model, introduced by the Google team, addresses challenges in training large language models by reimagining the BERT architecture. It achieves improved scalability compared to BERT, with 18 times fewer parameters and 1.7 times faster training for Albert. The key modifications include factorized embedding parameterization, cross-layer parameter sharing, and the introduction of Sentence-Order Prediction (SOP) to replace the Next Sentence Prediction (NSP) task. Factorized embedding reduces the vocabulary-embedding matrix, enhancing parameter efficiency. Cross-layer parameter sharing prevents parameter explosion in deeper networks. SOP loss focuses on inter-sentence coherence instead of NSP's topic prediction, providing finer-grained distinctions at the discourse level. These innovations contribute to Albert's enhanced performance and efficiency in language representation learning.","metadata":{}},{"cell_type":"code","source":"from transformers import BertConfig,BertModel","metadata":{"execution":{"iopub.status.busy":"2024-02-15T08:30:08.361774Z","iopub.execute_input":"2024-02-15T08:30:08.362228Z","iopub.status.idle":"2024-02-15T08:30:10.959622Z","shell.execute_reply.started":"2024-02-15T08:30:08.362193Z","shell.execute_reply":"2024-02-15T08:30:10.957314Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"bert_config=BertConfig()\nmodel=BertModel(bert_config)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T08:30:38.630611Z","iopub.execute_input":"2024-02-15T08:30:38.631197Z","iopub.status.idle":"2024-02-15T08:30:41.093911Z","shell.execute_reply.started":"2024-02-15T08:30:38.631145Z","shell.execute_reply":"2024-02-15T08:30:41.092535Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(f\"{model.num_parameters() /(10**6)}\\\nmillion parameters\")","metadata":{"execution":{"iopub.status.busy":"2024-02-15T08:30:54.343035Z","iopub.execute_input":"2024-02-15T08:30:54.343548Z","iopub.status.idle":"2024-02-15T08:30:54.354833Z","shell.execute_reply.started":"2024-02-15T08:30:54.343510Z","shell.execute_reply":"2024-02-15T08:30:54.352666Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"109.48224million parameters\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- The provided code snippet demonstrates the instantiation of a BERT-BASE model with specific configuration parameters using the Transformers library. The model has 12 layers, a hidden size of 768, and 12 attention heads, totaling 110 million parameters. The code then calculates and prints the number of parameters in million units, resulting in approximately 109.48 million parameters for the BERT-BASE model.","metadata":{}},{"cell_type":"markdown","source":"- In the context of BERT (Bidirectional Encoder Representations from Transformers), \"12 layers\" refers to the number of transformer layers in the model architecture. Each layer is a block of computations that processes the input data. The BERT model architecture consists of multiple identical layers stacked on top of each other. \n\nHere's a brief overview of what a single transformer layer typically consists of:\n\n1. **Self-Attention Mechanism:** This mechanism allows each word/token in the input sequence to focus on different parts of the sequence, capturing dependencies between words.\n\n2. **Feedforward Neural Network:** After the self-attention mechanism, the output passes through a feedforward neural network that processes each position independently.\n\n3. **Normalization:** Both the self-attention output and the output from the feedforward network undergo layer normalization.\n\nBERT-BASE specifically has 12 of these identical layers stacked on top of each other. This stacking allows the model to capture complex patterns and relationships within the input data. The parameters within each layer are learned during the training process to enable the model to generate representations that are useful for various natural language processing tasks.","metadata":{}},{"cell_type":"markdown","source":"In the context of neural networks, the \"hidden size\" refers to the number of neurons or units in the hidden layers of the network. ","metadata":{}},{"cell_type":"markdown","source":"## The 110 million parameters in the context of the BERT-BASE model configuration typically include various trainable weights and biases throughout the entire model architecture. These parameters are learned during the training process to enable the model to capture and represent patterns in the input data. Here's a breakdown of what these parameters generally include:\n\n1. **Embedding Parameters:** Parameters associated with the input embeddings for each token in the vocabulary. This involves mapping each token to a continuous vector representation.\n\n2. **Attention Parameters:** Parameters associated with the self-attention mechanism, including weights and biases for each attention head in each layer. These parameters allow the model to learn how to attend to different parts of the input sequence.\n\n3. **Feedforward Neural Network Parameters:** Parameters for the feedforward neural network within each transformer layer, including weights and biases. This network processes the output of the attention mechanism.\n\n4. **Layer Normalization Parameters:** Parameters for layer normalization applied after each sub-layer (e.g., after the attention mechanism and after the feedforward neural network).\n\n5. **Positional Encoding Parameters:** If used, parameters for the positional encoding that helps the model understand the order of tokens in the input sequence.\n\n6. **Output Projection Parameters:** Parameters associated with projecting the final representations to the desired output dimensionality.\n\nThese parameters collectively make up the model's capacity to represent complex relationships and patterns in natural language data. The specific breakdown may vary based on the exact implementation and configuration details of the BERT model.","metadata":{}},{"cell_type":"markdown","source":"# ALBERT MODEL","metadata":{}},{"cell_type":"code","source":"from transformers import AlbertConfig,AlbertModel,AlbertTokenizer\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T08:42:21.248407Z","iopub.execute_input":"2024-02-15T08:42:21.248860Z","iopub.status.idle":"2024-02-15T08:42:21.257091Z","shell.execute_reply.started":"2024-02-15T08:42:21.248824Z","shell.execute_reply":"2024-02-15T08:42:21.255687Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"albert_config=AlbertConfig(hidden_size=768,\nnum_attention_heads=12,\nintermediate_size=3072,)\nmodel=AlbertModel(albert_config)","metadata":{"execution":{"iopub.status.busy":"2024-02-15T08:40:10.422460Z","iopub.execute_input":"2024-02-15T08:40:10.422928Z","iopub.status.idle":"2024-02-15T08:40:10.742779Z","shell.execute_reply.started":"2024-02-15T08:40:10.422895Z","shell.execute_reply":"2024-02-15T08:40:10.741430Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(f\"{model.num_parameters() /(10**6)}million parameters\")","metadata":{"execution":{"iopub.status.busy":"2024-02-15T08:40:21.278437Z","iopub.execute_input":"2024-02-15T08:40:21.279026Z","iopub.status.idle":"2024-02-15T08:40:21.285993Z","shell.execute_reply.started":"2024-02-15T08:40:21.278986Z","shell.execute_reply":"2024-02-15T08:40:21.284572Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"11.683584million parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\nmodel = AlbertModel.from_pretrained(\"albert-base-v2\")\ntext = \"The cat is so sad .\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T08:42:23.644641Z","iopub.execute_input":"2024-02-15T08:42:23.645109Z","iopub.status.idle":"2024-02-15T08:42:45.097356Z","shell.execute_reply.started":"2024-02-15T08:42:23.645076Z","shell.execute_reply":"2024-02-15T08:42:45.096349Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a29e86438a1d497188735d239e4f565a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9c5b62fe43d43599ee0721c07a285c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c0d11a7c1d94729a5147d75bd3b7cf0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fc46a20fdcb4360bc86ff5a6fa77b85"}},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import pipeline\nfillmask= pipeline('fill-mask', model='albert-base-v2')\npd.DataFrame(fillmask(\"The cat is so [MASK] .\"))","metadata":{"execution":{"iopub.status.busy":"2024-02-15T08:43:57.589885Z","iopub.execute_input":"2024-02-15T08:43:57.590375Z","iopub.status.idle":"2024-02-15T08:43:58.156435Z","shell.execute_reply.started":"2024-02-15T08:43:57.590341Z","shell.execute_reply":"2024-02-15T08:43:58.155155Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"      score  token     token_str                     sequence\n0  0.281032  10901          cute          the cat is so cute.\n1  0.094895  26354      adorable      the cat is so adorable.\n2  0.042963   1700         happy         the cat is so happy.\n3  0.040976   5066         funny         the cat is so funny.\n4  0.024234  28803  affectionate  the cat is so affectionate.","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n      <th>token</th>\n      <th>token_str</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.281032</td>\n      <td>10901</td>\n      <td>cute</td>\n      <td>the cat is so cute.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.094895</td>\n      <td>26354</td>\n      <td>adorable</td>\n      <td>the cat is so adorable.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.042963</td>\n      <td>1700</td>\n      <td>happy</td>\n      <td>the cat is so happy.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.040976</td>\n      <td>5066</td>\n      <td>funny</td>\n      <td>the cat is so funny.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.024234</td>\n      <td>28803</td>\n      <td>affectionate</td>\n      <td>the cat is so affectionate.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# RoBerta","metadata":{}},{"cell_type":"markdown","source":"- Robustly Optimized BERT pre-training Approach (RoBERTa) is another popular BERT\n    reimplementation. It has provided many more improvements in training strategy than architectural\n    design. It outperformed BERT in almost all individual tasks on GLUE. Dynamic masking is one of its\n    original design choices. Although static masking is better for some tasks, the RoBERTa team showed\n    that dynamic masking can perform well for overall performances. Let's compare the changes from\n    BERT and summarize all the features as follows:\n    The changes in architecture are as follows:\n    Removing the next sentence prediction training objective\n    Dynamically changing the masking patterns instead of static masking, which is done by generating masking patterns whenever\n    they feed a sequence to the model\n    BPE sub-word tokenizer\n    The changes in training are as follows:\n    Controlling the training data: More data is used, such as 160 GB instead of the 16 GB originally used in BERT. Not only the size\n    of the data but the quality and diversity were taken into consideration in the study.\n    Longer iterations of up to 500K pretraining steps.\n    A longer batch size.\n    Longer sequences, which leads to less padding.\n    A large 50K BPE vocabulary instead of a 30K BPE vocabulary.\n    Thanks to the Transformers uniform API, as in the Albert model","metadata":{}},{"cell_type":"markdown","source":"+ It uses BPE tokenizer","metadata":{}},{"cell_type":"markdown","source":"# See electra ","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-02-15T08:48:13.249018Z","iopub.execute_input":"2024-02-15T08:48:13.249479Z","iopub.status.idle":"2024-02-15T08:48:13.262526Z","shell.execute_reply.started":"2024-02-15T08:48:13.249448Z","shell.execute_reply":"2024-02-15T08:48:13.260469Z"},"trusted":true},"execution_count":22,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[22], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    that dynamic masking can perform well for overall performances. Let's compare the changes from\u001b[0m\n\u001b[0m                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 5)\n"],"ename":"SyntaxError","evalue":"unterminated string literal (detected at line 5) (2541047105.py, line 5)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}