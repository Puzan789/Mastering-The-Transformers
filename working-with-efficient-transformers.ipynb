{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport transformers\nimport tensorflow as tf\nfrom sentence_transformers import SentenceTransformer\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-19T04:43:10.773965Z","iopub.execute_input":"2024-02-19T04:43:10.774440Z","iopub.status.idle":"2024-02-19T04:43:12.394919Z","shell.execute_reply.started":"2024-02-19T04:43:10.774399Z","shell.execute_reply":"2024-02-19T04:43:12.393823Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!pip install sentence-transformers\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T04:42:54.739102Z","iopub.execute_input":"2024-02-19T04:42:54.739450Z","iopub.status.idle":"2024-02-19T04:43:07.787376Z","shell.execute_reply.started":"2024-02-19T04:42:54.739425Z","shell.execute_reply":"2024-02-19T04:43:07.786059Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.37.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.24.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.20.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.3.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<b>So far, you have learned how to design a Natural Language Processing (NLP) architecture toachieve successful task performance with transformers. In this chapter, you will learn how to make efficient models out of trained models using distillation, pruning, and quantization.</b>","metadata":{}},{"cell_type":"markdown","source":"- An experimental setup using a typical GPU with 16 GB can handle the sentences of 512 tokens for training and inference.\n    However, longer entries can cause problems.","metadata":{}},{"cell_type":"markdown","source":"Yes, you are correct. Model size reduction can indeed be achieved through three main approaches: knowledge distillation, pruning, and quantization.\n\n1. **Knowledge Distillation:**\n   - Knowledge distillation involves training a smaller model (student) to mimic the behavior of a larger pre-trained model (teacher). The idea is to transfer the knowledge learned by the larger model to the smaller one. This process helps in reducing the size of the model while preserving its performance to a certain extent.\n\n2. **Pruning:**\n   - Pruning involves removing certain connections or parameters from the neural network based on their importance. This can be done during or after the training process. Pruning techniques identify and eliminate redundant or less important weights, leading to a sparser model with reduced size. Pruning can be magnitude-based, sensitivity-based, or use other criteria to determine the importance of weights.\n\n3. **Quantization:**\n   - Quantization involves reducing the precision of the weights and activations in a neural network. Typically, deep learning models use 32-bit floating-point numbers to represent weights and activations, but quantization reduces these to lower bit precision (e.g., 8-bit integers). This results in a smaller memory footprint and faster inference, albeit with a slight reduction in model accuracy.\n\nBy employing these techniques individually or in combination, practitioners can significantly reduce the size of neural network models, making them more suitable for deployment on resource-constrained devices or environments where computational resources are limited. Each approach has its strengths and trade-offs, and the choice of which method to use may depend on the specific requirements of the application.","metadata":{}},{"cell_type":"markdown","source":"- DistilBert is 1.7x compressed and 1.6x faster with 97% relative performance (compared to original BERT).\n- Mini-BERT is 6x compressed, 3x faster, and has 98% relative performance.\n- TinyBERT is 7.5x compressed, has 9.4x speed, and 97% relative performance.","metadata":{}},{"cell_type":"markdown","source":"- In the context of neural network training, the gradients are calculated during the backward pass of the training process using techniques like backpropagation. The gradient indicates the direction and magnitude in which the parameters should be adjusted to reduce the loss.","metadata":{}},{"cell_type":"markdown","source":"## Pruning transformers\nPruning includes the process of setting weights at each layer to zero based on a pre-specified criterion.\nFor example, a simple pruning algorithm could take the weights of each layer and set those that are\nbelow a threshold. This method eliminates weights that are very low in value and do not affect the\nresults too much.\nLikewise, we prune some redundant parts of the transformer network. The pruned networks are more\nlikely to generalize better than the original one. We have seen a successful pruning operation because\nthe pruning process probably keeps the true underlying explanatory factors and discards the redundant\nsubnetwork. But we need to still train a large network. The reasonable strategy is that we train a neural\nnetwork as large as possible. Then, the less salient weights or units whose removals have a small effect\non the model performance are discarded.\nThere are two approaches:\n* <b>Unstructured pruning</b>: \nwhere individual weights with a small saliency (or the least weight magnitude) are removed no matter\nwhich part of the neural network they are located in.\n* <b>Structured pruning</b>:\nthis approach prunes heads or layers.","metadata":{}},{"cell_type":"markdown","source":"# L1 pruning","metadata":{}},{"cell_type":"code","source":"distilroberta = SentenceTransformer('stsb-distilroberta-base-v2')","metadata":{"execution":{"iopub.status.busy":"2024-02-18T06:43:02.094556Z","iopub.execute_input":"2024-02-18T06:43:02.095187Z","iopub.status.idle":"2024-02-18T06:43:33.143552Z","shell.execute_reply.started":"2024-02-18T06:43:02.095062Z","shell.execute_reply":"2024-02-18T06:43:33.142225Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e6c16f2a6894f0faba7cc18710a99dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f2c0855dc8f4ee699b6aaa4638d2032"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.75k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67523298aacf4088943fba9841e0f777"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceab0ceefade447ea9a323baf07a1236"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/680 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9df65ce8e44847b0889a37ed92bb6a75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a99b16b594b44d8a35c9f7f1ae41dd8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ef055c4c173421da9941d7041bf271c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f4eaa29a4f74d4b86e421285146df33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e27a7bd01834287b3dbc4daf1b98b00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dd48bc6e1f5448a8f5965ed4f2c94fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c0bbf5b3a404aaaa321c719333a4e79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bb58ad7688344a4915f6d3c34679baf"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_metric,load_dataset\nstsb_metric = load_metric('glue', 'stsb')\nstsb = load_dataset('glue', 'stsb')\nmrpc_metric = load_metric('glue', 'mrpc')\nmrpc = load_dataset('glue','mrpc')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-18T06:43:42.015642Z","iopub.execute_input":"2024-02-18T06:43:42.016143Z","iopub.status.idle":"2024-02-18T06:43:54.716095Z","shell.execute_reply.started":"2024-02-18T06:43:42.016004Z","shell.execute_reply":"2024-02-18T06:43:54.714706Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.84k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4963f46ca3644a6fbe41e20830a139ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.78k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"451f8ac97a4849dba114bf86c6eca7d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/4.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3893b33562b042d9bb90476e6d4270e5"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset glue/stsb (download: 784.05 KiB, generated: 1.09 MiB, post-processed: Unknown size, total: 1.86 MiB) to /root/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/803k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aef53386cad549a588880348b30324fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/5749 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1379 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3420a5ceb7bb43088b2b503f38853db6"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c39e0b32214f491eb95309f10acf6ca0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"804c2c68425c438e9515f07eb4e6212f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d0ea20134074bd698a366f2353216c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ed3d8bb082d43cf902f285b4fbf709b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"409602028434418291d5b953d110bc65"}},"metadata":{}}]},{"cell_type":"code","source":"import math\n","metadata":{"execution":{"iopub.status.busy":"2024-02-18T06:43:56.481169Z","iopub.execute_input":"2024-02-18T06:43:56.482268Z","iopub.status.idle":"2024-02-18T06:43:56.489164Z","shell.execute_reply.started":"2024-02-18T06:43:56.482219Z","shell.execute_reply":"2024-02-18T06:43:56.487543Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def roberta_sts_benchmark(batch):\n    sts_encode1=tf.nn.l2_normalize(distilroberta.encode(batch['sentence1']),axis=1)\n    sts_encode2=tf.nn.l2_normalize(distilroberta.encode(batch['sentences2']),axis=1)\n    cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n    clip_cosine_similarities = tf.clip_by_value(cosine_similarities,-1.0,1.0)\n    scores = 1.0 -tf.acos(clip_cosine_similarities) / math.pi\n    return scores","metadata":{"execution":{"iopub.status.busy":"2024-02-18T06:48:24.776622Z","iopub.execute_input":"2024-02-18T06:48:24.777060Z","iopub.status.idle":"2024-02-18T06:48:24.783880Z","shell.execute_reply.started":"2024-02-18T06:48:24.776991Z","shell.execute_reply":"2024-02-18T06:48:24.782617Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"approx_min_k","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross-Lingual and Multilingual Language Modeling","metadata":{}},{"cell_type":"markdown","source":"- - > These are the objectives used for monolingual models. So, what can be done for cross-lingual models?\n    The answer is TLM, which is very similar to MLM, with a few changes. Instead of giving a sentence\n    from a single language, a sentence pair is given to a model in different languages, separated by a special\n    token. The model is required to predict the masked tokens, which are randomly masked in any of these\n    languages.","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nunmasker=pipeline('fill-mask',model=\"bert-base-multilingual-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-02-19T05:05:24.780838Z","iopub.execute_input":"2024-02-19T05:05:24.781168Z","iopub.status.idle":"2024-02-19T05:05:51.247747Z","shell.execute_reply.started":"2024-02-19T05:05:24.781140Z","shell.execute_reply":"2024-02-19T05:05:51.246923Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-19 05:05:32.450667: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-19 05:05:32.450764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-19 05:05:32.594316: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b5ff54883c5439b98d92c553e355f45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/672M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"693a72f2d5f040ae99610c7d3e45b9bf"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0020d9506a934f91b231f0650fdb642e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ea33597ae8646d392b36faf5bb7c48e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85c3bb1fd697487ea28164d9395fed4a"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences=[\n\"Transformers changed the [MASK] language processing\",\n\"Transformerlar [MASK] dil işlemeyi değiştirdiler\",\n\"ترنسفرمرها پردازش زبان [MASK] را تغییر دادند\"\n]","metadata":{"execution":{"iopub.status.busy":"2024-02-19T05:10:48.919589Z","iopub.execute_input":"2024-02-19T05:10:48.920313Z","iopub.status.idle":"2024-02-19T05:10:48.924302Z","shell.execute_reply.started":"2024-02-19T05:10:48.920274Z","shell.execute_reply":"2024-02-19T05:10:48.923369Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"for sentence in sentences:\n    print(sentence)\n    print(unmasker(sentence)[0][\"sequence\"])\n    print(\"=\"*50)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T05:12:22.125355Z","iopub.execute_input":"2024-02-19T05:12:22.125700Z","iopub.status.idle":"2024-02-19T05:12:22.544276Z","shell.execute_reply.started":"2024-02-19T05:12:22.125674Z","shell.execute_reply":"2024-02-19T05:12:22.543348Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Transformers changed the [MASK] language processing\ntransformers changed the english language processing\n==================================================\nTransformerlar [MASK] dil işlemeyi değiştirdiler\ntransformerlar bu dil islemeyi degistirdiler\n==================================================\nترنسفرمرها پردازش زبان [MASK] را تغییر دادند\nترنسفرمرها پردازش زبانی را تغییر دادند\n==================================================\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline\nunmasker=pipeline('fill-mask',model=\"xlm-roberta-base\")","metadata":{"execution":{"iopub.status.busy":"2024-02-19T05:37:40.700332Z","iopub.execute_input":"2024-02-19T05:37:40.700774Z","iopub.status.idle":"2024-02-19T05:37:46.583879Z","shell.execute_reply.started":"2024-02-19T05:37:40.700741Z","shell.execute_reply":"2024-02-19T05:37:46.582265Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"sentences=[\n\"Transformers changed the [MASK] language processing\",\n\"Transformerlar [MASK] dil işlemeyi değiştirdiler\",\n\"ترنسفرمرها پردازش زبان [mask] را تغییر دادند\"\n]","metadata":{"execution":{"iopub.status.busy":"2024-02-19T05:37:52.115076Z","iopub.execute_input":"2024-02-19T05:37:52.115571Z","iopub.status.idle":"2024-02-19T05:37:52.122470Z","shell.execute_reply.started":"2024-02-19T05:37:52.115534Z","shell.execute_reply":"2024-02-19T05:37:52.120537Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"for sentence in sentences:\n    print(sentence)\n    print(unmasker(sentence)[0][\"sequence\"])\n    print(\"=\"*50)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T05:37:54.646355Z","iopub.execute_input":"2024-02-19T05:37:54.646810Z","iopub.status.idle":"2024-02-19T05:37:54.799394Z","shell.execute_reply.started":"2024-02-19T05:37:54.646778Z","shell.execute_reply":"2024-02-19T05:37:54.797651Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Transformers changed the [MASK] language processing\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPipelineException\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sentence)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43munmasker\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/fill_mask.py:270\u001b[0m, in \u001b[0;36mFillMaskPipeline.__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    249\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    Fill the masked token in the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m        - **token_str** (`str`) -- The predicted token (to replace the masked one).\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1162\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1155\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1156\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[1;32m   1160\u001b[0m     )\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1168\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1168\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1170\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/fill_mask.py:123\u001b[0m, in \u001b[0;36mFillMaskPipeline.preprocess\u001b[0;34m(self, inputs, return_tensors, tokenizer_kwargs, **preprocess_parameters)\u001b[0m\n\u001b[1;32m    120\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(inputs, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_exactly_one_mask_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_inputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/fill_mask.py:112\u001b[0m, in \u001b[0;36mFillMaskPipeline.ensure_exactly_one_mask_token\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 112\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_exactly_one_mask_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/fill_mask.py:100\u001b[0m, in \u001b[0;36mFillMaskPipeline._ensure_exactly_one_mask_token\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     98\u001b[0m numel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mprod(masked_index\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numel \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineException(\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfill-mask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbase_model_prefix,\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo mask_token (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmask_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) found on the input\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    104\u001b[0m     )\n","\u001b[0;31mPipelineException\u001b[0m: No mask_token (<mask>) found on the input"],"ename":"PipelineException","evalue":"No mask_token (<mask>) found on the input","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}